{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "harris = cv2.imread('./Images/harris.JPG')\n",
    "harris = cv2.cvtColor(harris, cv2.COLOR_BGR2RGB)\n",
    "harris = np.array(harris)\n",
    "\n",
    "sl = cv2.imread('./Images/sl.jpg')\n",
    "sl = cv2.cvtColor(sl, cv2.COLOR_BGR2RGB)\n",
    "sl = np.array(sl)\n",
    "\n",
    "sm = cv2.imread('./Images/sm.jpg')\n",
    "sm = cv2.cvtColor(sm, cv2.COLOR_BGR2RGB)\n",
    "sm = np.array(sm)\n",
    "\n",
    "sr = cv2.imread('./Images/sr.jpg')\n",
    "sr = cv2.cvtColor(sr, cv2.COLOR_BGR2RGB)\n",
    "sr = np.array(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot(rows_count, column_count, images, titles):\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(rows_count, column_count, i+1)\n",
    "        plt.title(titles[i])\n",
    "        plt.imshow(images[i], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, title=''):\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Features  \n",
    "7.1. Harris Corner Detector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1.1. Extract interest points using the Harris Corner detector that you implemented. In this way, apply the Harris Corner detector for at least 4 different scales. Which interest points do you observe to be detected across all these different scales? Notice that your implementation should allow for any suitable scale as input, however you can show results on a minimum of 4 different scales (Test on harris.JPG Image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2. Scene stitching with SIFT/SURF features  \n",
    "7.2.1. Use the OpenCV, Python, and MATLAB implementation of the SIFT or SURF operator to find interest points and establish correspondences between the images. In this case you can directly compare the feature vectors of interest points. You will match and align between different views of a scene with SIFT/SURF features. Discuss results and demonstrates the output of each method separately (Test on sl,sm,sr.jpg images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbf2781d33d86aaf9937f6fffc4a56fd74337a108eb49da47e23eb69816886db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
